services:
  tgi:
    image: ghcr.io/huggingface/text-generation-inference:${TGI_VERSION}
    container_name: ${MODEL_NAME}
    restart: unless-stopped
    privileged: true
    cap_add:
      - sys_nice
    devices:
      - /dev/dri:/dev/dri
    ipc: host
    shm_size: ${SHM_SIZE}
    ports:
      - "${PORT}:80"
    networks:
      - tgi_net
    environment:
      - MODEL_NAME=${MODEL_NAME}
      - MODEL_ID=${MODEL_ID}
      - MAX_CONCURRENT_REQUESTS=${MAX_CONCURRENT_REQUESTS}
      - MAX_BATCH_SIZE=${MAX_BATCH_SIZE}
      - MAX_TOTAL_TOKENS=${MAX_TOTAL_TOKENS}
      - MAX_INPUT_LENGTH=${MAX_INPUT_LENGTH}
      - MAX_WAITING_TOKENS=${MAX_WAITING_TOKENS}
    healthcheck:
      test: ["CMD-SHELL", "wget --no-verbose --tries=1 --spider http://localhost:80/v1/models || exit 1"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 40s
    command: >
      --model-id ${MODEL_ID}
      --dtype bfloat16
      --max-concurrent-requests ${MAX_CONCURRENT_REQUESTS}
      --max-batch-size ${MAX_BATCH_SIZE}
      --max-total-tokens ${MAX_TOTAL_TOKENS}
      --max-input-length ${MAX_INPUT_LENGTH}
      --max-waiting-tokens ${MAX_WAITING_TOKENS}
      --cuda-graphs 0
      --port 80
      --json-output

networks:
  tgi_net:
    name: ${MODEL_NAME}_network
    driver: bridge
