services:
  tgi:
    image: ghcr.io/huggingface/text-generation-inference:${TGI_VERSION:-2.4.0-intel-xpu}
    container_name: ${MODEL_NAME:-qwen-7b-tgi}
    restart: unless-stopped
    privileged: true
    cap_add:
      - sys_nice
    devices:
      - /dev/dri:/dev/dri
    ipc: host
    shm_size: ${SHM_SIZE:-16g}
    ports:
      - "${PORT:-8083}:80"
    networks:
      - tgi_net
    environment:
      - MODEL_NAME=${MODEL_NAME:-qwen-7b-tgi}
      - MODEL_ID=${MODEL_ID:-Qwen/Qwen-7B}
      - MAX_CONCURRENT_REQUESTS=${MAX_CONCURRENT_REQUESTS:-100}
      - MAX_BATCH_SIZE=${MAX_BATCH_SIZE:-8}
      - MAX_TOTAL_TOKENS=${MAX_TOTAL_TOKENS:-4096}
      - MAX_INPUT_LENGTH=${MAX_INPUT_LENGTH:-2048}
      - MAX_WAITING_TOKENS=${MAX_WAITING_TOKENS:-20}
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:80/health"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 40s
    command: >
      --model-id ${MODEL_ID:-Qwen/Qwen-7B}
      --dtype bfloat16
      --max-concurrent-requests ${MAX_CONCURRENT_REQUESTS:-100}
      --max-batch-size ${MAX_BATCH_SIZE:-8}
      --max-total-tokens ${MAX_TOTAL_TOKENS:-4096}
      --max-input-length ${MAX_INPUT_LENGTH:-2048}
      --max-waiting-tokens ${MAX_WAITING_TOKENS:-20}
      --cuda-graphs 0
      --port 80
      --json-output

networks:
  tgi_net:
    name: ${MODEL_NAME:-qwen-7b-tgi}_network
    driver: bridge