services:
  tgi:
    image: ghcr.io/huggingface/text-generation-inference:${TGI_VERSION:-2.4.0-intel-xpu}
    container_name: ${MODEL_NAME:-flan-ul2-tgi}
    restart: unless-stopped
    privileged: true
    cap_add:
      - sys_nice
    devices:
      - /dev/dri:/dev/dri
    ipc: host
    shm_size: ${SHM_SIZE:-16g}
    ports:
      - "${PORT:-8083}:80"
    networks:
      - tgi_net
    environment:
      - MODEL_NAME=${MODEL_NAME:-flan-ul2-tgi}
      - MODEL_ID=${MODEL_ID:-google/flan-ul2}
      - MAX_CONCURRENT_REQUESTS=${MAX_CONCURRENT_REQUESTS:-100}
      - MAX_BATCH_SIZE=${MAX_BATCH_SIZE:-8}
      - MAX_TOTAL_TOKENS=${MAX_TOTAL_TOKENS:-4096}
      - MAX_INPUT_LENGTH=${MAX_INPUT_LENGTH:-2048}
      - MAX_WAITING_TOKENS=${MAX_WAITING_TOKENS:-20}
    healthcheck:
      test: ["CMD-SHELL", "wget --no-verbose --tries=1 --spider http://localhost:80/v1/models || exit 1"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 40s
    command: >
      --model-id ${MODEL_ID:-google/flan-ul2}
      --dtype bfloat16
      --max-concurrent-requests ${MAX_CONCURRENT_REQUESTS:-10}
      --max-batch-size ${MAX_BATCH_SIZE:-2}
      --max-total-tokens ${MAX_TOTAL_TOKENS:-4096}
      --max-input-length ${MAX_INPUT_LENGTH:-2030}
      --max-waiting-tokens ${MAX_WAITING_TOKENS:-20}
      --cuda-graphs 0
      --port 80
      --json-output

networks:
  tgi_net:
    name: ${MODEL_NAME:-flan-ul2-tgi}_network
    driver: bridge
